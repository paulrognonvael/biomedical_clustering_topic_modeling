{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b_LSTM_autoencoder_rep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "hide_code_all_hidden": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polrgn/biomedical_clustering_topic_modeling/blob/main/2b_LSTM_autoencoder_rep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "tYPhRTchgvl2"
      },
      "source": [
        "# Load libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "oexvL5E4HLZY"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Input,Dense, Dropout, LSTM, GRU, Reshape, TimeDistributed, RepeatVector, Embedding, Bidirectional, Masking, BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "FEy6_ClUgzbN"
      },
      "source": [
        "Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "v2yEwkJKHWF9"
      },
      "source": [
        "# Mounting Google drive where we save our data and embeddings\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "WDT8dBs2g2ZW"
      },
      "source": [
        "Load our clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hideCode": false,
        "hidePrompt": false,
        "id": "9-ooi-W_H-eK",
        "outputId": "1123b1c2-b065-47d5-fe25-e495295133cb"
      },
      "source": [
        "abstract_df = pd.read_csv('Data/abstract_df_clean_stopwords_lang_abstrlen.csv',index_col=0)[0:10000]\n",
        "np.shape(abstract_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "swsAg__Ig7pS"
      },
      "source": [
        "# Pre-processing dataset for the autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "eLnnmncfhCqF"
      },
      "source": [
        "Fit the tokenizer on our dataset and save it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hideCode": false,
        "hidePrompt": false,
        "id": "EfCn1arCxP6d",
        "outputId": "80f0ff9b-8ec2-4ca0-a093-a5b0dd7e4660"
      },
      "source": [
        "# Fit the tokenizer and build the word index\n",
        "t= Tokenizer(num_words=None)\n",
        "t.fit_on_texts(abstract_df.abstract)\n",
        "pickle.dump(t, open(\"Outputs/autoencoder_vectorizer.pickle\", \"wb\"))\n",
        "word_index = t.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 49554 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "jjymCFk2hFf2"
      },
      "source": [
        "Save the vocabulary to build the embedding layer after. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "L_c607HRxfZZ"
      },
      "source": [
        "word_index_df = pd.DataFrame(word_index.items())\n",
        "word_index_df.columns=['word','index']\n",
        "word_index_df['index'] = word_index_df['index']-1\n",
        "word_index_df.head()\n",
        "word_index_df.to_csv('Outputs/word_index.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "asQ85WilhOYb"
      },
      "source": [
        "Read the pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "3W5Kh7Tfy7WN"
      },
      "source": [
        "# Extract the words vectors from BioWordVec embeddings txt file\n",
        "embeddings_index = {}\n",
        "f = open('Data/BioWordVec_PubMed_MIMICIII_d200.txt')\n",
        "for line in f:\n",
        "   values = line.split()\n",
        "   word = values[0]\n",
        "   coefs = np.asarray(values[1:], dtype='float32')\n",
        "   embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "Build our embedding matrix by matching the tokens (words) in our dataset to\n",
        "the pre-trained embeddings.\n",
        "\n",
        "Code inspired from\n",
        "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "\n",
        "words_in_data = []\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
        "for word, i in word_index.items():\n",
        "   embedding_vector = embeddings_index.get(word)\n",
        "   if embedding_vector is not None:\n",
        "       # words not found in embedding index will be all-zeros.\n",
        "       embedding_matrix[i] = embedding_vector\n",
        "       words_in_data.append(word)\n",
        "\n",
        "Save our embedding matrix\n",
        "np.savetxt('Outputs/embedding_matrix.csv', embedding_matrix, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "D_WcZvOozIwr"
      },
      "source": [
        "# Load our saved embedding matrix\n",
        "embedding_matrix = np.loadtxt('Outputs/embedding_matrix.csv',delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "sHZIcEYrhUfF"
      },
      "source": [
        "Build the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "XVMFuAl4SMHE"
      },
      "source": [
        "maxlen = max(abstract_df.word_count)\n",
        "EMBEDDING_DIM = np.shape(embedding_matrix)[1]\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length= maxlen,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "8T3rGkx0hfuZ"
      },
      "source": [
        "Tokenize, pad and embed the abstracts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "ZbwCSAR6SZqh"
      },
      "source": [
        "tokenized= t.texts_to_sequences(abstract_df.abstract)\n",
        "padded_abstract = pad_sequences(tokenized, maxlen=maxlen)\n",
        "embedded_abstract= embedding_layer(padded_abstract)\n",
        "np.shape(embedded_abstract)\n",
        "np.save(\"Outputs/embedded_abstract\", embedded_abstract)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "YH91aNNiWACi"
      },
      "source": [
        "embedded_abstract = np.load('Outputs/embedded_abstract.npy')\n",
        "np.save(\"Outputs/embedded_abstract_10000\", embedded_abstract[0:10000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "9HW1nWPZdESi"
      },
      "source": [
        "embedded_abstract = np.load('Outputs/embedded_abstract_10000.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "A3h0stwTicov"
      },
      "source": [
        "Reconstructing a (maxlen x 200) array for each document is an extremely difficult task for the autoencoder. To ease the learning, the target for the autoencoder will be the averaged embedded abstracts (averaged over tokens, we get 200 dimensional vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "PtcMo433nlR5"
      },
      "source": [
        "y = np.sum(embedded_abstract, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "_-qwUafKi1ZD"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "5YMDdwGRi8ZK"
      },
      "source": [
        "We build a simple autoencoder with a single LSTM layer to encode the embedded abstracts and 3 dense layers of increasing sizes to decode them. We find tanh activations and cosine similarity loss yield the smoothest learning process. Cosine similarity is particularly suited because our embedded abstracts are sparse (tokens not presents in the pre-trained embeddings, padding). We did not run an extensive search over hyperparameters but after many rounds of trials and errors, Adam optimizer, 30 epochs and batch size of 1000 seemed appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "hideCode": false,
        "hidePrompt": false,
        "id": "nEBJkIecHWwW",
        "outputId": "7e989515-7774-48a7-c163-c3aa7c81280b"
      },
      "source": [
        "EMBEDDING_DIM = 200\n",
        "maxlen = max(abstract_df.word_count)\n",
        "latent_dim = 64\n",
        "\n",
        "input_texts = Input(shape=(maxlen, EMBEDDING_DIM,))\n",
        "encoded = LSTM(latent_dim, activation='tanh',return_sequences=False)(input_texts)\n",
        "decoded = Dense(64, activation='tanh')(encoded)\n",
        "decoded = Dense(128, activation='tanh')(decoded)\n",
        "decoded = Dense(200, activation='tanh')(decoded)\n",
        "autoencoder = Model(input_texts, decoded)\n",
        "encoder = Model(input_texts, encoded)\n",
        "\n",
        "autoencoder.summary()\n",
        "\n",
        "batch_size = 1000\n",
        "autoencoder.compile(optimizer='adam', loss=\"cosine_similarity\")\n",
        "history = autoencoder.fit(embedded_abstract[0:10000], y, batch_size=batch_size,\n",
        "                          epochs=60,validation_split=.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 231, 200)]        0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                67840     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 200)               25800     \n",
            "=================================================================\n",
            "Total params: 106,120\n",
            "Trainable params: 106,120\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "9/9 [==============================] - 46s 5s/step - loss: -0.5152 - val_loss: -0.8097\n",
            "Epoch 2/60\n",
            "9/9 [==============================] - 46s 5s/step - loss: -0.8610 - val_loss: -0.8936\n",
            "Epoch 3/60\n",
            "9/9 [==============================] - 46s 5s/step - loss: -0.8957 - val_loss: -0.9024\n",
            "Epoch 4/60\n",
            "9/9 [==============================] - 45s 5s/step - loss: -0.9033 - val_loss: -0.9088\n",
            "Epoch 5/60\n",
            "9/9 [==============================] - 45s 5s/step - loss: -0.9086 - val_loss: -0.9131\n",
            "Epoch 6/60\n",
            "9/9 [==============================] - 40s 4s/step - loss: -0.9123 - val_loss: -0.9158\n",
            "Epoch 7/60\n",
            "9/9 [==============================] - 43s 5s/step - loss: -0.9149 - val_loss: -0.9181\n",
            "Epoch 8/60\n",
            "9/9 [==============================] - 44s 5s/step - loss: -0.9176 - val_loss: -0.9209\n",
            "Epoch 9/60\n",
            "9/9 [==============================] - 45s 5s/step - loss: -0.9208 - val_loss: -0.9241\n",
            "Epoch 10/60\n",
            "9/9 [==============================] - 40s 4s/step - loss: -0.9241 - val_loss: -0.9273\n",
            "Epoch 11/60\n",
            "9/9 [==============================] - 39s 4s/step - loss: -0.9271 - val_loss: -0.9299\n",
            "Epoch 12/60\n",
            "9/9 [==============================] - 45s 5s/step - loss: -0.9298 - val_loss: -0.9322\n",
            "Epoch 13/60\n",
            "9/9 [==============================] - 37s 4s/step - loss: -0.9320 - val_loss: -0.9343\n",
            "Epoch 14/60\n",
            "9/9 [==============================] - 48s 5s/step - loss: -0.9340 - val_loss: -0.9360\n",
            "Epoch 15/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9356 - val_loss: -0.9375\n",
            "Epoch 16/60\n",
            "9/9 [==============================] - 49s 5s/step - loss: -0.9371 - val_loss: -0.9389\n",
            "Epoch 17/60\n",
            "9/9 [==============================] - 39s 4s/step - loss: -0.9385 - val_loss: -0.9402\n",
            "Epoch 18/60\n",
            "9/9 [==============================] - 52s 6s/step - loss: -0.9397 - val_loss: -0.9414\n",
            "Epoch 19/60\n",
            "9/9 [==============================] - 50s 6s/step - loss: -0.9410 - val_loss: -0.9426\n",
            "Epoch 20/60\n",
            "9/9 [==============================] - 52s 6s/step - loss: -0.9422 - val_loss: -0.9436\n",
            "Epoch 21/60\n",
            "9/9 [==============================] - 48s 5s/step - loss: -0.9432 - val_loss: -0.9447\n",
            "Epoch 22/60\n",
            "9/9 [==============================] - 49s 5s/step - loss: -0.9443 - val_loss: -0.9456\n",
            "Epoch 23/60\n",
            "9/9 [==============================] - 42s 5s/step - loss: -0.9453 - val_loss: -0.9466\n",
            "Epoch 24/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9463 - val_loss: -0.9474\n",
            "Epoch 25/60\n",
            "9/9 [==============================] - 50s 6s/step - loss: -0.9473 - val_loss: -0.9484\n",
            "Epoch 26/60\n",
            "9/9 [==============================] - 44s 5s/step - loss: -0.9482 - val_loss: -0.9493\n",
            "Epoch 27/60\n",
            "9/9 [==============================] - 43s 5s/step - loss: -0.9492 - val_loss: -0.9503\n",
            "Epoch 28/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9501 - val_loss: -0.9510\n",
            "Epoch 29/60\n",
            "9/9 [==============================] - 46s 5s/step - loss: -0.9509 - val_loss: -0.9519\n",
            "Epoch 30/60\n",
            "9/9 [==============================] - 48s 5s/step - loss: -0.9518 - val_loss: -0.9527\n",
            "Epoch 31/60\n",
            "9/9 [==============================] - 45s 5s/step - loss: -0.9526 - val_loss: -0.9535\n",
            "Epoch 32/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9534 - val_loss: -0.9542\n",
            "Epoch 33/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9542 - val_loss: -0.9550\n",
            "Epoch 34/60\n",
            "9/9 [==============================] - 43s 5s/step - loss: -0.9549 - val_loss: -0.9556\n",
            "Epoch 35/60\n",
            "9/9 [==============================] - 44s 5s/step - loss: -0.9556 - val_loss: -0.9563\n",
            "Epoch 36/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9562 - val_loss: -0.9568\n",
            "Epoch 37/60\n",
            "9/9 [==============================] - 46s 5s/step - loss: -0.9568 - val_loss: -0.9574\n",
            "Epoch 38/60\n",
            "9/9 [==============================] - 45s 5s/step - loss: -0.9574 - val_loss: -0.9579\n",
            "Epoch 39/60\n",
            "9/9 [==============================] - 51s 6s/step - loss: -0.9580 - val_loss: -0.9584\n",
            "Epoch 40/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9585 - val_loss: -0.9588\n",
            "Epoch 41/60\n",
            "9/9 [==============================] - 50s 6s/step - loss: -0.9590 - val_loss: -0.9593\n",
            "Epoch 42/60\n",
            "9/9 [==============================] - 47s 5s/step - loss: -0.9594 - val_loss: -0.9598\n",
            "Epoch 43/60\n",
            "9/9 [==============================] - 52s 6s/step - loss: -0.9599 - val_loss: -0.9601\n",
            "Epoch 44/60\n",
            "9/9 [==============================] - 38s 4s/step - loss: -0.9603 - val_loss: -0.9605\n",
            "Epoch 45/60\n",
            "9/9 [==============================] - 46s 5s/step - loss: -0.9607 - val_loss: -0.9609\n",
            "Epoch 46/60\n",
            "9/9 [==============================] - 29s 3s/step - loss: -0.9611 - val_loss: -0.9613\n",
            "Epoch 47/60\n",
            "9/9 [==============================] - 25s 3s/step - loss: -0.9615 - val_loss: -0.9616\n",
            "Epoch 48/60\n",
            "9/9 [==============================] - 25s 3s/step - loss: -0.9618 - val_loss: -0.9620\n",
            "Epoch 49/60\n",
            "9/9 [==============================] - 29s 3s/step - loss: -0.9621 - val_loss: -0.9623\n",
            "Epoch 50/60\n",
            "9/9 [==============================] - 30s 3s/step - loss: -0.9625 - val_loss: -0.9626\n",
            "Epoch 51/60\n",
            "9/9 [==============================] - 30s 3s/step - loss: -0.9628 - val_loss: -0.9629\n",
            "Epoch 52/60\n",
            "9/9 [==============================] - 29s 3s/step - loss: -0.9631 - val_loss: -0.9632\n",
            "Epoch 53/60\n",
            "9/9 [==============================] - 25s 3s/step - loss: -0.9634 - val_loss: -0.9635\n",
            "Epoch 54/60\n",
            "9/9 [==============================] - 24s 3s/step - loss: -0.9637 - val_loss: -0.9638\n",
            "Epoch 55/60\n",
            "9/9 [==============================] - 24s 3s/step - loss: -0.9640 - val_loss: -0.9640\n",
            "Epoch 56/60\n",
            "9/9 [==============================] - 25s 3s/step - loss: -0.9643 - val_loss: -0.9643\n",
            "Epoch 57/60\n",
            "9/9 [==============================] - 32s 4s/step - loss: -0.9646 - val_loss: -0.9646\n",
            "Epoch 58/60\n",
            "9/9 [==============================] - 26s 3s/step - loss: -0.9648 - val_loss: -0.9648\n",
            "Epoch 59/60\n",
            "9/9 [==============================] - 28s 3s/step - loss: -0.9651 - val_loss: -0.9650\n",
            "Epoch 60/60\n",
            "9/9 [==============================] - 31s 3s/step - loss: -0.9653 - val_loss: -0.9653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "ITbEOP2VHW6U"
      },
      "source": [
        "autoencoder.save('Outputs/autoencoder_64.h5') \n",
        "encoder.save('Outputs/encoder_64.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "hGpCLAGLk74S"
      },
      "source": [
        "We plot the training history, see the validation and training loss are eventually stable and similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "hideCode": false,
        "hidePrompt": false,
        "id": "NuC154a6HW9L",
        "outputId": "c4e1d633-ad7f-40c6-c850-e24b2e83ca9b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training MSE',c='r')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('Outputs/ae_training_hist.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hideCode": false,
        "hidePrompt": false,
        "id": "Zh08pX7RlQ8M"
      },
      "source": [
        "We save our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "hideCode": false,
        "hidePrompt": false,
        "id": "h6CigLnxHXPv",
        "outputId": "e3fe6c95-50f4-460c-b8ba-a3b34b77c827"
      },
      "source": [
        "abstract_df = pd.read_csv('Outputs/abstract_df_clean_stopwords_lang_abstrlen.csv',index_col=0)\n",
        "array_encoded_abstracts = encoder.predict(embedded_abstract)\n",
        "encoded_abstracts = pd.concat((abstract_df[0:10000],pd.DataFrame(array_encoded_abstracts,index=abstract_df.index.values[0:10000])),axis=1)\n",
        "encoded_abstracts.to_csv('Outputs/ae_encoded_abstract_df_64.csv')\n",
        "encoded_abstracts.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cord_uid</th>\n",
              "      <th>abstract</th>\n",
              "      <th>word_count</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>...</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>353391</th>\n",
              "      <td>sng9h1kq</td>\n",
              "      <td>challenging find genes stable transcripts refe...</td>\n",
              "      <td>128</td>\n",
              "      <td>-0.025721</td>\n",
              "      <td>0.517121</td>\n",
              "      <td>-0.479899</td>\n",
              "      <td>-0.605407</td>\n",
              "      <td>0.068654</td>\n",
              "      <td>0.673719</td>\n",
              "      <td>0.536603</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080616</td>\n",
              "      <td>0.604826</td>\n",
              "      <td>-0.080643</td>\n",
              "      <td>-0.203930</td>\n",
              "      <td>-0.028942</td>\n",
              "      <td>-0.386709</td>\n",
              "      <td>0.642630</td>\n",
              "      <td>-0.059549</td>\n",
              "      <td>-0.432383</td>\n",
              "      <td>0.443774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48166</th>\n",
              "      <td>h9v5jqfc</td>\n",
              "      <td>etiology injury diverse multifactorial autopsy...</td>\n",
              "      <td>82</td>\n",
              "      <td>-0.016891</td>\n",
              "      <td>0.302273</td>\n",
              "      <td>-0.651515</td>\n",
              "      <td>-0.561381</td>\n",
              "      <td>0.010714</td>\n",
              "      <td>0.663162</td>\n",
              "      <td>0.557620</td>\n",
              "      <td>...</td>\n",
              "      <td>0.077701</td>\n",
              "      <td>0.211173</td>\n",
              "      <td>-0.224173</td>\n",
              "      <td>-0.292526</td>\n",
              "      <td>-0.205760</td>\n",
              "      <td>-0.668778</td>\n",
              "      <td>0.711367</td>\n",
              "      <td>-0.005670</td>\n",
              "      <td>-0.470185</td>\n",
              "      <td>0.133148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5918</th>\n",
              "      <td>k2crv9qb</td>\n",
              "      <td>angiotensin-converting enzyme ace considered p...</td>\n",
              "      <td>129</td>\n",
              "      <td>-0.000256</td>\n",
              "      <td>0.548198</td>\n",
              "      <td>-0.670758</td>\n",
              "      <td>-0.648719</td>\n",
              "      <td>0.019095</td>\n",
              "      <td>0.740356</td>\n",
              "      <td>0.519471</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021436</td>\n",
              "      <td>0.833036</td>\n",
              "      <td>-0.425552</td>\n",
              "      <td>-0.284744</td>\n",
              "      <td>-0.222868</td>\n",
              "      <td>-0.831387</td>\n",
              "      <td>0.702441</td>\n",
              "      <td>-0.036318</td>\n",
              "      <td>-0.491804</td>\n",
              "      <td>0.417141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360052</th>\n",
              "      <td>ofutm6pc</td>\n",
              "      <td>efficacy povidone-iodine pvp-i products number...</td>\n",
              "      <td>85</td>\n",
              "      <td>-0.028173</td>\n",
              "      <td>0.192283</td>\n",
              "      <td>-0.459530</td>\n",
              "      <td>-0.671187</td>\n",
              "      <td>0.020363</td>\n",
              "      <td>0.756021</td>\n",
              "      <td>0.595849</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025621</td>\n",
              "      <td>0.513213</td>\n",
              "      <td>-0.108698</td>\n",
              "      <td>-0.379482</td>\n",
              "      <td>-0.158120</td>\n",
              "      <td>-0.641619</td>\n",
              "      <td>0.671276</td>\n",
              "      <td>-0.024645</td>\n",
              "      <td>-0.444761</td>\n",
              "      <td>0.126058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77312</th>\n",
              "      <td>baog4f4h</td>\n",
              "      <td>retinoid acid receptor orphan receptor alpha r...</td>\n",
              "      <td>107</td>\n",
              "      <td>0.013979</td>\n",
              "      <td>0.720491</td>\n",
              "      <td>-0.608755</td>\n",
              "      <td>-0.521759</td>\n",
              "      <td>0.053027</td>\n",
              "      <td>0.670722</td>\n",
              "      <td>0.593300</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059974</td>\n",
              "      <td>0.425503</td>\n",
              "      <td>-0.072745</td>\n",
              "      <td>-0.255448</td>\n",
              "      <td>-0.545533</td>\n",
              "      <td>-0.519312</td>\n",
              "      <td>0.719914</td>\n",
              "      <td>-0.052568</td>\n",
              "      <td>-0.517625</td>\n",
              "      <td>-0.135090</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 67 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        cord_uid                                           abstract  \\\n",
              "353391  sng9h1kq  challenging find genes stable transcripts refe...   \n",
              "48166   h9v5jqfc  etiology injury diverse multifactorial autopsy...   \n",
              "5918    k2crv9qb  angiotensin-converting enzyme ace considered p...   \n",
              "360052  ofutm6pc  efficacy povidone-iodine pvp-i products number...   \n",
              "77312   baog4f4h  retinoid acid receptor orphan receptor alpha r...   \n",
              "\n",
              "        word_count         0         1         2         3         4  \\\n",
              "353391         128 -0.025721  0.517121 -0.479899 -0.605407  0.068654   \n",
              "48166           82 -0.016891  0.302273 -0.651515 -0.561381  0.010714   \n",
              "5918           129 -0.000256  0.548198 -0.670758 -0.648719  0.019095   \n",
              "360052          85 -0.028173  0.192283 -0.459530 -0.671187  0.020363   \n",
              "77312          107  0.013979  0.720491 -0.608755 -0.521759  0.053027   \n",
              "\n",
              "               5         6  ...        54        55        56        57  \\\n",
              "353391  0.673719  0.536603  ...  0.080616  0.604826 -0.080643 -0.203930   \n",
              "48166   0.663162  0.557620  ...  0.077701  0.211173 -0.224173 -0.292526   \n",
              "5918    0.740356  0.519471  ...  0.021436  0.833036 -0.425552 -0.284744   \n",
              "360052  0.756021  0.595849  ...  0.025621  0.513213 -0.108698 -0.379482   \n",
              "77312   0.670722  0.593300  ...  0.059974  0.425503 -0.072745 -0.255448   \n",
              "\n",
              "              58        59        60        61        62        63  \n",
              "353391 -0.028942 -0.386709  0.642630 -0.059549 -0.432383  0.443774  \n",
              "48166  -0.205760 -0.668778  0.711367 -0.005670 -0.470185  0.133148  \n",
              "5918   -0.222868 -0.831387  0.702441 -0.036318 -0.491804  0.417141  \n",
              "360052 -0.158120 -0.641619  0.671276 -0.024645 -0.444761  0.126058  \n",
              "77312  -0.545533 -0.519312  0.719914 -0.052568 -0.517625 -0.135090  \n",
              "\n",
              "[5 rows x 67 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}